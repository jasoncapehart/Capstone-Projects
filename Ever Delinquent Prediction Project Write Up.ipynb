{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ever Delinquent Prediction Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Problem Statement and Summary of Key Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict a given loan's ever-delinquent probability based on some loan characteristics. In the primary mortgage insurance (PMI) industry, knowing the likelihood of delinquency upfront will help insurance companies properly price the premium and determine the reserve amount to cover potential loss caused by claims.\n",
    "\n",
    "The key takeaways are as follows:\n",
    "\n",
    "1) This project is a typical classification problem. Among different machine learning algorithms (Logistic Regression, SVM, Kernel SVM, Random Forest and Decision Tree), logistic regression outperformed the rest with 0.685 AUROC score and 69% accuracy score. This result also translate into 4% accuracy improvement compared to the threshold/baseline model which only uses borrower's credit score (FICO) to predict ever delinquent probablity. In other words, adding additional features increased the power of model prediction in this problem.\n",
    "\n",
    "2) Loan characteristics such as FICO, Debt to Income Ratio (DTI), Loan to Value Ratio (LTV), Number of Borrowers, Property Type, Housing Occupancy Status, First-time Home Buyer (Y/N) and Loan Purpose are selected features in my predictive model. \n",
    "\n",
    "3) Since loans characteristics are given by lenders upfront before insurance underwriting process, creating this model will help the business understand the likelihood of loan's ever-delinquent status, thus provide lenders the pricing option which the company feels comfortable with (p.s. pricing scorecard is based on expected claim rate. The higher the expected claim rate, the higher the insurance premium price.)\n",
    "\n",
    "4) The outcome of the predictive model could also be used in calculating reserve dollar amount on a given loan to cover potential claim loss. Below is just a simplified calculation, and in real world, we will also consider other factors such as reinsurance deals, REO, claim documents completeness etc. \n",
    "\n",
    "$$Rserve $ Amount = Original~UPB * MI~Coverage * Ever~Delinquent~Probability * Claim~Probability~on~Ever~Delinquent$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population of this project meets the following criteria:\n",
    "- Loans closed after year 2010 to avoid the data noise from financial crisis (2007-2008)\n",
    "- Loans closed before 2013 so we have at least 4-year loan performance data to collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 481,218 rows and 19 columns in my original data pull, but I ended up keeping **10** columns/features before I started modeling. The columns I dropped fit in either one of the following criteria:\n",
    "\n",
    "- ID columns\n",
    "- Values under a particular column are nearly identical (>99% of population share the same value)\n",
    "\n",
    "*See below regarding **7 categorical features'** distribution *\n",
    "\n",
    "\n",
    "| Orig Yr| % to All   |Claim %|\n",
    "|--------------|-----------|---------|\n",
    "|2010        |   10.6%   |0.6%\n",
    "|2011       |   15.0%   |0.3%\n",
    "|2012       |   34.5%   |0.1%\n",
    "|2013       |   40.0%   |0.1%\n",
    "\n",
    "| Loan Purp     | % to All  \n",
    "|--------------|-----------\n",
    "| Refi Cash Out       |   2.6%   \n",
    "| Refi Payoff Lien      |  32.4%   \n",
    "| Purchase    |  65.1%   \n",
    "\n",
    "| Property Type     | % to All  \n",
    "|--------------|-----------\n",
    "| Co-op or Condo       |   9.8%   \n",
    "| Manufacutre Housing    |  0.3%   \n",
    "| Single Fam    |  89.9%   \n",
    "\n",
    "| Occupancy Status     | % to All  \n",
    "|--------------|-----------\n",
    "| Primary Resident     |   96.5%   \n",
    "| Secondary Resident    |  3.5% \n",
    "\n",
    "| First Time Home Buyer     | % to All  \n",
    "|--------------|-----------\n",
    "| Y     |   31.7%   \n",
    "| N    |  68.3% \n",
    "\n",
    "| Multi Borrower    | % to All  \n",
    "|--------------|-----------\n",
    "| Y     |   49.2%   \n",
    "| N    |  50.8% \n",
    "\n",
    "| MI Channel    | % to All  \n",
    "|--------------|-----------\n",
    "| Delegated     |   66.7%   \n",
    "| Non-Delegated    |  33.3% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** features (FICO, DTI and LTV) are continuous data originally. However, in order to make dummy variable creation later on more doable, I slightly manipulated the data and grouped the values into different buckets. Now those 3 features become categorical data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_image](files/1.png)\n",
    "![my_image](files/3.png)\n",
    "![my_image](files/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to unbalanced data when considering delinquent target (2.5% delinquent vs. 97.5% never delinquent loans), I downsampled the \"never delinquent\" population. At end of the data exploration step, I kept 10,000 loans for modeling: **5,000 never delinquent and 5,000 ever delinquent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection, Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression outperformed SVM, Decision Tree and Random Forest algorithms in predicting ever-delinquent probablities and resulted in **68.6% accuracy score**. See below as the outcome from Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ggplot import *\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "df_eval = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "ggplot(df_eval, aes(x='fpr', y='tpr')) +\\\n",
    "      geom_line() +\\\n",
    "      geom_abline(linetype='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_image](files/download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies1 = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "#mean\n",
    "accuracies1.mean()\n",
    "#variance\n",
    "accuracies1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "accuracy mean: 0.68612388710894023\n",
    "accuracy std: 0.013107884669360552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I also performed Lasso (L1) and Ridge (L2) regularization after fitting the training set in the model. It turns out the accuracy is very close to the one before regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression with Regularization to the Training Set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty = 'l2', C =100,random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies1 = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "accuracies1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.68537408215296181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I will add the bell curve charts later after we discuss the message. I cannot fully getting what I see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps/Future Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create utility codes for future use: model fitting, model evaluation and regularization pieces \n",
    "- This model is under a fairly simplified assumpsion that ever-delinquent relate to claim rate. But in the real world, a loan's claim status is also related to how long a loan gets into delinquent (60-days, 120-days. 360-days etc.) and how often a loan gets into delinquent. The transactional data will be helpful to answer this question. \n",
    "- Some other features might be relevant in predicting ever delinquent probablity: property state, property value, borrower years of employment, self-employed Y/N etc. Some of the data is not available for me to use today, but it is not a bad idea to put my thoughts out there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
