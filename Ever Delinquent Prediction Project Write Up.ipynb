{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ever Delinquent Prediction Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Problem Statement and Summary of Key Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to predict a given loan's ever-delinquent probability based on some loan characteristics. In the mortgage insurance (MI) industry, knowing the likelihood of delinquency upfront will help insurance companies properly price the premium and determine the reserve amount in case of occurrence of claim events.\n",
    "\n",
    "The key takeaways are as follows:\n",
    "\n",
    "1) This project is a typical classification problem. Among different machine learning algorithms (Logistic Regression, SVM, Kernel SVM, Random Forest and Decision Tree), logistic regression outperformed the rest with 0.685 AUROC score and 69% accuracy score. This result also translate into 4% accuracy improvement compared to the threshold/baseline model which only uses borrower's credit score (FICO) to predict ever delinquent probablity. In other words, adding additional features increased the power of model prediction in this problem.\n",
    "\n",
    "2) Loan characteristics such as FICO, Debt to Income Ratio (DTI), Loan to Value Ratio (LTV), Number of Borrowers, Property Type, Housing Occupancy Status, First-time Home Buyer (Y/N) and Loan Purpose are selected features in my predictive model. \n",
    "\n",
    "3) Since loans characteristics are given by lenders upfront before insurance underwriting process, creating this model will help the business understand the likelihood of loan's ever-delinquent status, thus provide lenders the pricing option which the company feels comfortable with (p.s. pricing scorecard is based on expected claim rate. The higher the expected claim rate, the higher the insurance premium price.)\n",
    "\n",
    "4) The outcome of the predictive model could also be used in calculating reserve dollar amount on a given loan to cover potential claim loss. Below is just a simplified calculation, and in real world, we will also consider other factors such as reinsurance deals, REO, claim documents completeness etc. \n",
    "\n",
    "$$Rserve $ Amount = Original~UPB * MI~Coverage * Ever~Delinquent~Probability * Claim~Probability~on~Ever~Delinquent$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pulled the base population from Radian's database with the following criteria:\n",
    "- Vintage loans closed after 2010 to avoid financial crisis (2007-2008) which can be the noise in the predictive model.\n",
    "- Vintage loans closed before 2013 so that there are enough time to see their performance until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 481,218 loans in total from my data pull.\n",
    "- **10 features** were chosen during DEA step, **7** of which are categorical data. The reasons I dropped the rest columns are due to *1) unique indentifier column such as loan number 2) values under the columns are almost identical (>99%).* See the summary table of the selected features' distribution below:\n",
    "\n",
    "\n",
    "| Orig Yr| % to All   |Claim %|\n",
    "|--------------|-----------|---------|\n",
    "|2010        |   10.6%   |0.6%\n",
    "|2011       |   15.0%   |0.3%\n",
    "|2012       |   34.5%   |0.1%\n",
    "|2013       |   40.0%   |0.1%\n",
    "\n",
    "| Loan Purp     | % to All  \n",
    "|--------------|-----------\n",
    "| Refi Cash Out       |   2.6%   \n",
    "| Refi Payoff Lien      |  32.4%   \n",
    "| Purchase    |  65.1%   \n",
    "\n",
    "| Property Type     | % to All  \n",
    "|--------------|-----------\n",
    "| Co-op or Condo       |   9.8%   \n",
    "| Manufacutre Housing    |  0.3%   \n",
    "| Single Fam    |  89.9%   \n",
    "\n",
    "| Occupancy Status     | % to All  \n",
    "|--------------|-----------\n",
    "| Primary Resident     |   96.5%   \n",
    "| Secondary Resident    |  3.5% \n",
    "\n",
    "| First Time Home Buyer     | % to All  \n",
    "|--------------|-----------\n",
    "| Y     |   31.7%   \n",
    "| N    |  68.3% \n",
    "\n",
    "| Multi Borrower    | % to All  \n",
    "|--------------|-----------\n",
    "| Y     |   49.2%   \n",
    "| N    |  50.8% \n",
    "\n",
    "| MI Channel    | % to All  \n",
    "|--------------|-----------\n",
    "| Delegated     |   66.7%   \n",
    "| Non-Delegated    |  33.3% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another **3** features (FICO, DTI and LTV) are continuous data originally. However, in order to make dummy variable creation later on more doable, I slightly manipulated the data and grouped the values into different buckets. Now those 3 features become categorical data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_image](files/1.png)\n",
    "![my_image](files/3.png)\n",
    "![my_image](files/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I also downsampled the \"never deliquent\" loans from the entire population due to skewed data (2.5% delinquent vs. 97.5% never delinquent loans). So before creating the model, I modified the population (training and test set combined) to 10,000 : **5,000 never delinquent and 5,000 ever delinquent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection, Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression outperformed SVM, Decision Tree and Random Forest algorithms in predicting ever-delinquent probablities and resulted in **68.6% accuracy score**. See below as the outcome from Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ggplot import *\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "df_eval = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "ggplot(df_eval, aes(x='fpr', y='tpr')) +\\\n",
    "      geom_line() +\\\n",
    "      geom_abline(linetype='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![my_image](files/download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies1 = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "#mean\n",
    "accuracies1.mean()\n",
    "#variance\n",
    "accuracies1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "accuracy mean: 0.68612388710894023\n",
    "accuracy std: 0.013107884669360552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I also performed Lasso (L1) and Ridge (L2) regularization after fitting the training set in the model. It turns out the accuracy is very close to the one before regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression with Regularization to the Training Set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty = 'l2', C =100,random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Applying K-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies1 = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "accuracies1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.68537408215296181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I will add the bell curve charts later after we discuss the message. I cannot fully getting what I see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps/Future Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create utility codes for future use: model fitting, model evaluation and regularization pieces \n",
    "- This model is under a fairly simplified assumpsion that ever-delinquent relate to claim rate. But in the real world, a loan's claim status is also related to how long a loan gets into delinquent (60-days, 120-days. 360-days etc.) and how often a loan gets into delinquent. The transactional data will be helpful to answer this question. \n",
    "- Some other features might be relevant in predicting ever delinquent probablity: property state, property value, borrower years of employment, self-employed Y/N etc. Some of the data is not available for me to use today, but it is not a bad idea to put my thoughts out there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
